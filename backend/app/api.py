from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, Header
from sqlalchemy.orm import Session, aliased
from sqlalchemy import func
from datetime import datetime, date, timedelta
from typing import List
import csv
import os
import json
import re # å¼•å…¥æ­£åˆ™åº“
from dotenv import load_dotenv
from openai import OpenAI
from pydantic import BaseModel
import random

from .database import SessionLocal
from .model import Word, UserWordProgress, QuizMistake, UserGrammarAnalysis
from .schemas import WordDTO, StudySubmit, ArticleDTO, QuizItem, MistakeCreate, MistakeDTO, WritingSubmit, WritingDTO
from .srs_algo import calculate_review

from .model import Article, UserStats, UserWriting # è®°å¾—å¯¼å…¥

# 1. åŠ è½½æœ¬åœ° .env æ–‡ä»¶ (å¦åˆ™è¯»ä¸åˆ° API Key)
load_dotenv()

# 2. åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
# å³ä½¿éƒ¨ç½²åˆ°äº‘ç«¯ï¼Œè¿™æ®µä»£ç ä¹Ÿå…¼å®¹ï¼ˆäº‘ç«¯ä¼šè‡ªåŠ¨æ³¨å…¥ç¯å¢ƒå˜é‡ï¼Œæœ¬åœ°åˆ™è¯»å– .envï¼‰
client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url=os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
)

# ç­‰çº§é…ç½®ï¼šæ•°æ®åº“Tag -> AI Prompt æè¿°
LEVEL_CONFIG = {
    "zk": {"name": "ä¸­è€ƒ", "prompt": "Middle School Student (approx. 1500 vocabulary)"},
    "gk": {"name": "é«˜è€ƒ", "prompt": "High School Student (approx. 3500 vocabulary)"},
    "cet4": {"name": "CET-4", "prompt": "College Student (CET-4 level)"},
    "cet6": {"name": "CET-6", "prompt": "College Student (CET-6 level)"},
    "ky": {"name": "è€ƒç ”", "prompt": "Postgraduate Entrance Exam candidate"},
    "ielts": {"name": "é›…æ€", "prompt": "IELTS candidate (Band 7.0 target)"},
    "toefl": {"name": "æ‰˜ç¦", "prompt": "TOEFL candidate (Score 100+ target)"},
}

class GrammarRequest(BaseModel):
    sentence: str

class TargetUpdate(BaseModel):
    target: int

class BookmarkRequest(BaseModel):
    word_id: int

class LevelUpdate(BaseModel):
    level: str

router = APIRouter()

# ä¾èµ–æ³¨å…¥ï¼šè‡ªåŠ¨ä» Header è·å– User ID
def get_current_user_id(x_user_id: str = Header(...)):
    return x_user_id

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def run_import_task():
    print("ğŸš€ å¼€å§‹åå°å¯¼å…¥å•è¯ä»»åŠ¡...")
    csv_path = 'scripts/ecdict.csv' # Render ä¸Šæ–‡ä»¶è·¯å¾„æ˜¯ç›¸å¯¹äºæ ¹ç›®å½•çš„
    
    if not os.path.exists(csv_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {csv_path}")
        return

    db = SessionLocal()
    try:
        with open(csv_path, newline='', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            count = 0
            for row in reader:
                tags = row.get('tag', '')
                if 'zk' in tags or 'gk' in tags: # åªå¯¼å…¥ä¸­é«˜è€ƒ
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    existing = db.query(Word).filter(Word.spell == row['word']).first()
                    if not existing:
                        word = Word(
                            spell=row['word'],
                            phonetic=row['phonetic'],
                            definition=row['definition'],
                            translation=row['translation'],
                            exchange=row['exchange'],
                            tag=tags
                        )
                        db.add(word)
                        count += 1
                        if count % 100 == 0:
                            db.commit()
                            print(f"å·²å¯¼å…¥ {count} ...")
            db.commit()
            print(f"âœ… å¯¼å…¥å®Œæˆï¼å…± {count} ä¸ªå•è¯ã€‚")
    except Exception as e:
        print(f"âŒ å¯¼å…¥å‡ºé”™: {e}")
    finally:
        db.close()

@router.get("/user/dashboard")
def get_user_dashboard(db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    
    # 1. æ€»å…±å·²èƒŒå•è¯æ•° (is_learned = 1)
    total_learned = db.query(UserWordProgress).filter(
        UserWordProgress.user_id == user_id,
        UserWordProgress.is_learned == 1
    ).count()
    
    # 2. å‰©ä½™å¾…å¤ä¹ /æ–°è¯ (ä»Šæ—¥ä»»åŠ¡)
    # é€»è¾‘ï¼šæ‰¾å‡º next_review <= now çš„è¯ + è¿˜æ²¡èƒŒçš„æ–°è¯(è¿™é‡Œç®€å•æ¨¡æ‹Ÿä¸€ä¸‹ï¼Œå‡è®¾æ¯æ—¥å›ºå®š20ä¸ª)
    # ä¸ºäº† MVP ç®€å•å±•ç¤ºï¼Œæˆ‘ä»¬ç›´æ¥æŸ¥ "queue" æ¥å£åŒæ ·çš„é€»è¾‘ï¼Œçœ‹æœ‰å¤šå°‘ä¸ª
    daily_target = 15 # æš‚æ—¶å†™ä¸ªæ¨¡æ‹Ÿæ•°æ®ï¼Œæˆ–è€…ä½ å¯ä»¥å¤ç”¨ get_study_queue çš„è®¡æ•°é€»è¾‘
    
    # 3. çœŸå®ï¼šè·å–æ‰“å¡å¤©æ•° === ä¿®æ”¹äº†è¿™é‡Œ ===
    streak_days = 0
    daily_progress = 0 # é»˜è®¤ä¸º0
    current_level = "zk" # é»˜è®¤

    user_stats = db.query(UserStats).filter(UserStats.user_id == user_id).first()

    if user_stats:
        streak_days = user_stats.streak_days
        daily_target = user_stats.daily_target or 15 # è·å–æ•°æ®åº“é‡Œçš„ç›®æ ‡
        current_level = user_stats.current_level or "zk"
        # å¦‚æœæ•°æ®åº“é‡Œçš„æ—¥æœŸæ˜¯ä»Šå¤©ï¼Œå°±ç”¨æ•°æ®åº“çš„å€¼ï¼›å¦‚æœä¸æ˜¯ä»Šå¤©ï¼ˆè¯´æ˜ä»Šå¤©è¿˜æ²¡å­¦ï¼‰ï¼Œå°±æ˜¯0
        if user_stats.last_study_date and user_stats.last_study_date.date() == date.today():
            daily_progress = user_stats.daily_progress
        else:
            daily_progress = 0
    
    return {
        "total_learned": total_learned,
        "today_task": daily_target,
        "streak_days": streak_days,
        "vocabulary_limit": 880, # å‡è®¾æ˜¯ä¸­è€ƒå¤§çº²è¯æ±‡é‡
        "daily_progress": daily_progress, # <--- è¿”å›ç»™å‰ç«¯çš„æ–°å­—æ®µ
        "current_level": current_level,
        "level_display": LEVEL_CONFIG.get(current_level, {}).get("name", "ä¸­è€ƒ")
    }

# 1. è·å–å­¦ä¹ é˜Ÿåˆ— (æ–°è¯ + éœ€è¦å¤ä¹ çš„æ—§è¯)
@router.get("/study/queue", response_model=List[WordDTO])
def get_study_queue(db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    
    # 1. å…ˆæŸ¥ç”¨æˆ·çš„ç­‰çº§
    user_stats = db.query(UserStats).filter(UserStats.user_id == user_id).first()
    level_tag = user_stats.current_level if user_stats else "zk"

    # A. æ‰¾éœ€è¦å¤ä¹ çš„è¯ (next_review <= now)
    review_list = db.query(Word).join(UserWordProgress).filter(
        UserWordProgress.user_id == user_id,
        UserWordProgress.next_review <= datetime.utcnow()
    ).limit(20).all()

    # å¦‚æœå¤ä¹ è¯ä¸å¤Ÿ 10 ä¸ªï¼Œå°±åŠ ç‚¹æ–°è¯
    if len(review_list) < 10:
        limit_new = 10 - len(review_list)
        # æ‰¾è¿˜æ²¡æœ‰è¿›åº¦çš„è¯ (LEFT JOIN check)
        # å­æŸ¥è¯¢ï¼šæ‰¾å‡ºè¯¥ç”¨æˆ·å­¦è¿‡çš„ word_id
        subquery = db.query(UserWordProgress.word_id).filter(UserWordProgress.user_id == user_id)
        
        new_words = db.query(Word).filter(
            Word.id.notin_(subquery),
            Word.tag.contains(level_tag), # <--- å…³é”®ä¿®æ”¹ï¼šåªæ ¹æ®å½“å‰ç­‰çº§ç­›é€‰
            Word.ai_sentence != None  # åªå‡ºæœ‰ AI ä¾‹å¥çš„è¯
        ).limit(limit_new).all()
        
        review_list.extend(new_words)

    return review_list

# 2. æäº¤å­¦ä¹ ç»“æœ
@router.post("/study/submit")
def submit_study(data: StudySubmit, db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    
    # æŸ¥æ‰¾æˆ–åˆ›å»ºè¿›åº¦è®°å½•
    progress = db.query(UserWordProgress).filter(
        UserWordProgress.user_id == user_id,
        UserWordProgress.word_id == data.word_id
    ).first()

    if not progress:
        progress = UserWordProgress(
            user_id=user_id, 
            word_id=data.word_id, 
            easiness=2.5, 
            interval=0, 
            repetitions=0
        )
        db.add(progress)

    # è°ƒç”¨ç®—æ³•è®¡ç®—
    ef, interval, reps, next_date = calculate_review(
        data.quality, progress.easiness, progress.interval, progress.repetitions
    )

    # æ›´æ–°æ•°æ®åº“
    progress.easiness = ef
    progress.interval = interval
    progress.repetitions = reps
    progress.next_review = next_date
    progress.is_learned = 1
    
    # === å¤„ç†æ‰“å¡é€»è¾‘ ===
    today = date.today()

    # è·å–æˆ–åˆ›å»ºç”¨æˆ·ç»Ÿè®¡
    user_stats = db.query(UserStats).filter(UserStats.user_id == user_id).first()
    if not user_stats:
        user_stats = UserStats(user_id=user_id, streak_days=0, last_study_date=None, daily_progress=0)
        db.add(user_stats)

    last_date = user_stats.last_study_date.date() if user_stats.last_study_date else None

    if last_date == today:
        # ä»Šå¤©å·²ç»å­¦è¿‡äº†ï¼Œç´¯åŠ ä»Šæ—¥è¿›åº¦
        user_stats.daily_progress += 1
    elif last_date == today - timedelta(days=1):
        # æ˜¨å¤©æ‰“å¡äº†ï¼Œè¿ç»­å¤©æ•°+1
        user_stats.streak_days += 1
        user_stats.daily_progress = 1
        user_stats.last_study_date = datetime.utcnow()
    else:
        # æ–­ç­¾äº†ï¼ˆæˆ–è€…æ˜¯ç¬¬ä¸€æ¬¡ï¼‰ï¼ŒStreaké‡ç½®ä¸º1ï¼Œä»Šæ—¥è¿›åº¦é‡ç½®ä¸º1
        user_stats.streak_days = 1
        user_stats.daily_progress = 1
        user_stats.last_study_date = datetime.utcnow()

    db.commit()
    return {"status": "ok", "next_review": next_date}

@router.post("/reading/generate")
def generate_new_article(db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    # 1. è·å–ç­‰çº§
    user_stats = db.query(UserStats).filter(UserStats.user_id == user_id).first()
    level_tag = user_stats.current_level if user_stats else "zk"
    level_prompt = LEVEL_CONFIG[level_tag]["prompt"] # è·å– "IELTS candidate..."
    # 1. éšæœºé€‰è¯ (æ¨¡æ‹Ÿï¼šä»åº“é‡ŒéšæœºæŒ‘ 5 ä¸ªä¸­è€ƒè¯)
    # å®é™…äº§å“ä¸­ï¼Œè¿™é‡Œåº”è¯¥é€‰ç”¨æˆ·"åˆšåŠ å…¥ç”Ÿè¯æœ¬"çš„è¯
    words = db.query(Word).filter(Word.tag.contains(level_tag)).order_by(func.random()).limit(8).all()
    if not words:
        raise HTTPException(status_code=400, detail="Word database is empty")
        
    word_list_str = ", ".join([w.spell for w in words])
    word_ids = [w.id for w in words]
    
    print(f"ğŸ¤– Generating article with: {word_list_str}")

    # 2. è°ƒç”¨ DeepSeek
    prompt = f"""
    Write a short English article for a {level_prompt}.
    Difficulty Level: {level_tag.upper()}.
    It MUST include these words: {word_list_str}.
    
    Return strict JSON:
    {{
        "title": "Title Here",
        "content": "Story content...",
        "translation": "Chinese translation..."
    }}
    """
    
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        data = json.loads(response.choices[0].message.content)
        
        # 3. å­˜å…¥æ•°æ®åº“
        article = Article(
            title=data['title'],
            content=data['content'],
            translation=data.get('translation', ''), # å…¼å®¹æœ‰çš„AIæ²¡è¿”å›ç¿»è¯‘
            difficulty="Level 2",
            vocab_list=word_ids
        )
        db.add(article)
        db.commit()
        db.refresh(article)
        
        return {"status": "ok", "article_id": article.id}
        
    except Exception as e:
        print(f"Gen Article Error: {e}")
        raise HTTPException(status_code=500, detail="Failed to generate article")

@router.get("/reading/list", response_model=List[ArticleDTO])
def get_articles(db: Session = Depends(get_db)):
    return db.query(Article).order_by(Article.id.desc()).limit(10).all()

@router.get("/reading/{article_id}", response_model=ArticleDTO)
def get_article_detail(article_id: int, db: Session = Depends(get_db)):
    article = db.query(Article).filter(Article.id == article_id).first()
    if not article:
        raise HTTPException(status_code=404, detail="Article not found")
    return article

@router.post("/reading/{article_id}/quiz", response_model=List[QuizItem])
def generate_quiz(article_id: int, db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    # 1. è·å–ç­‰çº§
    user_stats = db.query(UserStats).filter(UserStats.user_id == user_id).first()
    level_tag = user_stats.current_level if user_stats else "zk"
    level_prompt = LEVEL_CONFIG[level_tag]["prompt"] # è·å– "IELTS candidate..."
    # 1. æŸ¥å‡ºæ–‡ç« 
    article = db.query(Article).filter(Article.id == article_id).first()
    if not article:
        raise HTTPException(status_code=404, detail="Article not found")

    # 2. è°ƒç”¨ DeepSeek
    print(f"ğŸ¤– AIæ­£åœ¨ä¸ºæ–‡ç«  {article.title} å‡ºé¢˜...") # åŠ ä¸ªæ—¥å¿—æ–¹ä¾¿è°ƒè¯•

    prompt = f"""
    Based on the text below, create 3 multiple-choice questions for a {level_prompt}.
    Difficulty Level: {level_tag.upper()}.
    Text:
    {article.content}

    You MUST return the result as a pure JSON list.
    Strict format requirements:
    1. Do not use Markdown formatting (no ```json or ```).
    2. The root element must be a LIST [].
    3. Each item must have: "question", "options" (list of 4 strings), "answer" (just A, B, C, or D), and "explanation".

    Example:
    [
      {{
        "question": "What is the main idea?",
        "options": ["A. Idea 1", "B. Idea 2", "C. Idea 3", "D. Idea 4"],
        "answer": "A",
        "explanation": "Because..."
      }}
    ]
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1, # é™ä½éšæœºæ€§ï¼Œä¿è¯æ ¼å¼ç¨³å®š
            response_format={"type": "json_object"} # å¼ºåˆ¶ JSON
        )
        content = response.choices[0].message.content
        print(f"ğŸ¤– AIåŸå§‹è¿”å›: {content}") # æ‰“å°å‡ºæ¥çœ‹çœ‹ï¼Œå¦‚æœæŠ¥é”™æ–¹ä¾¿æ’æŸ¥

        # === å¢å¼ºå‹ JSON æ¸…æ´—é€»è¾‘ ===
        # 1. æœ‰æ—¶å€™ AI è¿˜æ˜¯ä¼šè¿”å› ```jsonï¼Œæ‰‹åŠ¨å»æ‰
        if "```" in content:
            content = content.replace("```json", "").replace("```", "")

        # 2. å°è¯•è§£æ
        data = json.loads(content)

        # 3. å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœè¿”å›çš„æ˜¯ {"quizzes": [...]} æˆ–è€…æ˜¯ {"questions": [...]}
        if isinstance(data, dict):
            for key in ["quizzes", "questions", "items"]:
                if key in data and isinstance(data[key], list):
                    return data[key]
            # å¦‚æœæ˜¯å­—å…¸ä½†æ²¡æ‰¾åˆ° keyï¼Œå¯èƒ½ç»“æ„ä¸å¯¹ï¼Œå¼ºè¡Œè½¬ list è¯•è¯•?
            # è¿™é‡Œçš„ fallback è§†æƒ…å†µè€Œå®šï¼Œé€šå¸¸ä¸Šé¢èƒ½è§£å†³

        # 4. å¦‚æœæœ¬èº«å°±æ˜¯ listï¼Œç›´æ¥è¿”å›
        if isinstance(data, list):
            return data

        raise ValueError("AI returned unexpected JSON structure")

    except Exception as e:
        print(f"âŒ AI Error Details: {e}") # è¿™ä¸€è¡Œéå¸¸é‡è¦ï¼Œçœ‹ç»ˆç«¯æŠ¥é”™
        raise HTTPException(status_code=500, detail=f"AI generation failed: {str(e)}")

# 1. æ‰¹é‡ä¿å­˜é”™é¢˜ (åœ¨æµ‹éªŒç»“ç®—æ—¶è°ƒç”¨)
@router.post("/mistakes/batch_add")
def add_mistakes(mistakes: List[MistakeCreate], db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    for m in mistakes:
        # ç®€å•æŸ¥é‡ï¼šé˜²æ­¢åŒä¸€é“é¢˜é‡å¤å­˜ (å¯é€‰)
        exists = db.query(QuizMistake).filter(
            QuizMistake.user_id == user_id, 
            QuizMistake.question == m.question
        ).first()
        
        if not exists:
            new_mistake = QuizMistake(
                user_id=user_id,
                question=m.question,
                options=m.options,
                correct_answer=m.correct_answer,
                user_answer=m.user_answer,
                explanation=m.explanation,
                from_article_title=m.from_article_title
            )
            db.add(new_mistake)
    
    db.commit()
    return {"status": "ok", "saved_count": len(mistakes)}

# 2. è·å–æ‰€æœ‰é”™é¢˜
@router.get("/mistakes/list", response_model=List[MistakeDTO])
def get_mistakes(db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    return db.query(QuizMistake).filter(QuizMistake.user_id == user_id).order_by(QuizMistake.id.desc()).all()

# 3. ç§»é™¤é”™é¢˜ (å·²æŒæ¡)
@router.delete("/mistakes/{mistake_id}")
def delete_mistake(mistake_id: int, db: Session = Depends(get_db)):
    db.query(QuizMistake).filter(QuizMistake.id == mistake_id).delete()
    db.commit()
    return {"status": "deleted"}

# 1. æäº¤ä½œæ–‡å¹¶è·å– AI æ‰¹æ”¹
@router.post("/writing/evaluate", response_model=WritingDTO)
def evaluate_writing(data: WritingSubmit, db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    # 1. è·å–ç­‰çº§
    user_stats = db.query(UserStats).filter(UserStats.user_id == user_id).first()
    level_tag = user_stats.current_level if user_stats else "zk"
    level_prompt = LEVEL_CONFIG[level_tag]["prompt"] # è·å– "IELTS candidate..."

    print(f"ğŸ¤– æ­£åœ¨æ‰¹æ”¹ä½œæ–‡: {data.topic}")
    prompt = f"""
    Act as an English teacher. Evaluate the {level_prompt} essay.
    Difficulty Level: {level_tag.upper()}.
    Topic: {data.topic}
    Student Content: {data.content}
    
    Return strict JSON (no markdown code blocks):
    {{
        "score": 85, 
        "comment": "General feedback...",
        "corrections": [
            {{"original": "wrong text", "correction": "right text", "reason": "grammar rule"}}
        ],
        "better_version": "A rewritten native-like version..."
    }}
    """
    
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.1, # é™ä½éšæœºæ€§
            timeout=60 # ç»™ DeepSeek SDK æ›´å¤šæ—¶é—´
        )
        content = response.choices[0].message.content
        print(f"ğŸ“ AIåŸå§‹è¿”å›: {content}") # æ‰“å°å‡ºæ¥æ–¹ä¾¿è°ƒè¯•

        # === å¢å¼ºå‹ JSON æ¸…æ´— ===
        if "```" in content:
            content = content.replace("```json", "").replace("```", "")
        
        feedback = json.loads(content)
        
        # === å­˜å…¥æ•°æ®åº“ ===
        writing = UserWriting(
            user_id=user_id,
            topic=data.topic,
            original_content=data.content,
            ai_feedback=feedback
        )
        db.add(writing)
        db.commit()
        db.refresh(writing)
        
        return writing

    except Exception as e:
        print(f"âŒ AI Error: {e}") # è¿™ä¸€è¡Œèƒ½è®©ä½ çœ‹åˆ°å…·ä½“æŠ¥é”™
        raise HTTPException(status_code=500, detail=f"AI evaluation failed: {str(e)}")

# 2. è·å–å†™ä½œå†å²
@router.get("/writing/history", response_model=List[WritingDTO])
def get_writing_history(db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    return db.query(UserWriting).filter(UserWriting.user_id == user_id).order_by(UserWriting.id.desc()).all()

# 3. éšæœºç”Ÿæˆä¸€ä¸ªé¢˜ç›® (å¯é€‰å°åŠŸèƒ½)
@router.get("/writing/topic")
def get_random_topic(db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    # 1. è·å–ç­‰çº§
    user_stats = db.query(UserStats).filter(UserStats.user_id == user_id).first()
    level_tag = user_stats.current_level if user_stats else "zk"
    level_prompt = LEVEL_CONFIG[level_tag]["prompt"] # è·å– "IELTS candidate..."
    # åŸæ¥æ˜¯å†™æ­»çš„ listï¼Œç°åœ¨æ”¹æˆè°ƒç”¨ AI
    prompt = """
    Generate ONE creative and interesting English writing topic suitable for a {level_prompt}. 
    Difficulty Level: {level_tag.upper()}.
    Examples: "If I could fly", "My favorite season", "A day without phone".
    Return ONLY the topic string, no quotes, no extra words.
    """
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7 # ç¨å¾®é«˜ä¸€ç‚¹ï¼Œä¿è¯åˆ›æ„
        )
        topic = response.choices[0].message.content.strip().replace('"', '')
        return {"topic": topic}
    except:
        # å…œåº•æ–¹æ¡ˆ
        return {"topic": "My Best Friend"}

# 2. è¯­æ³•åˆ†ææ¥å£
@router.post("/grammar/analyze")
def analyze_grammar(req: GrammarRequest, db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    # 1. è·å–ç­‰çº§
    user_stats = db.query(UserStats).filter(UserStats.user_id == user_id).first()
    level_tag = user_stats.current_level if user_stats else "zk"
    level_prompt = LEVEL_CONFIG[level_tag]["prompt"] # è·å– "IELTS candidate..."
    print(f"ğŸ¤– æ­£åœ¨åˆ†æé•¿éš¾å¥: {req.sentence}")

    prompt = f"""
    You are an expert English grammar teacher. Analyze the syntax of the following sentence for a {level_prompt}.
    
    Sentence: "{req.sentence}"

    Return strict JSON (no markdown block):
    {{
      "translation": "Translate the sentence into natural Chinese.",
      "structure": [
        {{"part": "Subject (ä¸»è¯­)", "content": "The specific words", "color": "text-green-600", "bg": "bg-green-50"}},
        {{"part": "Verb (è°“è¯­)", "content": "The specific words", "color": "text-red-600", "bg": "bg-red-50"}},
        {{"part": "Object/Complement (å®¾/è¡¨)", "content": "The specific words", "color": "text-blue-600", "bg": "bg-blue-50"}},
        {{"part": "Modifier (ä¿®é¥°æˆåˆ†)", "content": "Time/Place/Clauses...", "color": "text-gray-600", "bg": "bg-gray-50"}}
      ],
      "grammar_points": [
        {{ "title": "Point name (e.g. å®šè¯­ä»å¥)", "desc": "Explanation..." }}
      ]
    }}
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.1
        )
        content = response.choices[0].message.content

        # æ¸…æ´— Markdown
        if "```" in content:
            content = content.replace("```json", "").replace("```", "")

        # åœ¨ return json.loads(content) ä¹‹å‰ï¼Œå­˜å…¥æ•°æ®åº“
        result_json = json.loads(content)
    
        # å­˜åº“
        record = UserGrammarAnalysis(
            user_id=user_id,
            sentence=req.sentence,
            analysis_result=result_json
        )
        db.add(record)
        db.commit()

        return result_json

    except Exception as e:
        print(f"Grammar AI Error: {e}")
        raise HTTPException(status_code=500, detail="Analysis failed")

# 2. æ–°å¢å†å²è®°å½•æ¥å£
@router.get("/grammar/history")
def get_grammar_history(db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    return db.query(UserGrammarAnalysis).filter(
        UserGrammarAnalysis.user_id == user_id
    ).order_by(UserGrammarAnalysis.id.desc()).all()

@router.post("/user/update_target")
def update_target(data: TargetUpdate, db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    user_stats = db.query(UserStats).filter(UserStats.user_id == user_id).first()
    if not user_stats:
        # å¦‚æœæ˜¯æ–°ç”¨æˆ·è¿˜æ²¡å­¦è¿‡ä¹ ï¼Œå…ˆåˆ›å»ºè®°å½•
        user_stats = UserStats(user_id=user_id, daily_target=data.target)
        db.add(user_stats)
    else:
        user_stats.daily_target = data.target
    
    db.commit()
    return {"status": "ok", "new_target": data.target}

@router.get("/word/lookup")
def lookup_word(spell: str, db: Session = Depends(get_db)):
    # å¿½ç•¥å¤§å°å†™æŸ¥æ‰¾
    word = db.query(Word).filter(Word.spell == spell.lower()).first()

    if not word:
        # å¦‚æœæ•°æ®åº“æ²¡æœ‰ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªç”Ÿåƒ»è¯æˆ–è€…å˜å½¢è¯(dolphins)
        # ç®€å•å¤„ç†ï¼šè¿”å›ç©ºï¼Œæˆ–è€…å¯ä»¥æ¥å…¥ DeepSeek å®æ—¶æŸ¥è¯¢ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
        return {"found": False, "spell": spell}

    return {
        "found": True,
        "id": word.id,
        "spell": word.spell,
        "phonetic": word.phonetic,
        "translation": word.translation,
        "definition": word.definition
    }

@router.post("/word/bookmark")
def bookmark_word(data: BookmarkRequest, db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨è¿›åº¦è¡¨ä¸­
    progress = db.query(UserWordProgress).filter(
        UserWordProgress.user_id == user_id,
        UserWordProgress.word_id == data.word_id
    ).first()
    
    if progress:
        return {"message": "Already in vocabulary list"}
    
    # å¦‚æœä¸åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªæ–°è®°å½• (is_learned=0 è¡¨ç¤ºæ–°è¯)
    new_progress = UserWordProgress(
        user_id=user_id,
        word_id=data.word_id,
        is_learned=0,
        easiness=2.5,
        interval=0,
        repetitions=0
    )
    db.add(new_progress)
    db.commit()
    return {"message": "Added to vocabulary"}

@router.post("/user/update_level")
def update_level(data: LevelUpdate, db: Session = Depends(get_db), user_id: str = Depends(get_current_user_id)):
    if data.level not in LEVEL_CONFIG:
        raise HTTPException(status_code=400, detail="Invalid level")
        
    user_stats = db.query(UserStats).filter(UserStats.user_id == user_id).first()
    if not user_stats:
        user_stats = UserStats(user_id=user_id, current_level=data.level)
        db.add(user_stats)
    else:
        user_stats.current_level = data.level
    
    db.commit()
    return {"status": "ok", "current_level": data.level, "level_name": LEVEL_CONFIG[data.level]["name"]}

@router.get("/admin/trigger_import")
def trigger_import(background_tasks: BackgroundTasks):
    # ä½¿ç”¨åå°ä»»åŠ¡è¿è¡Œï¼Œé˜²æ­¢è¯·æ±‚è¶…æ—¶
    background_tasks.add_task(run_import_task)
    return {"message": "æ­£åœ¨åå°å¯¼å…¥æ•°æ®ï¼Œè¯·æŸ¥çœ‹ Render æ—¥å¿—..."}

