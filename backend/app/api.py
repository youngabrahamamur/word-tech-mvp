from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session, aliased
from sqlalchemy import func
from datetime import datetime, date, timedelta
from typing import List
import csv
import os
import json
import re # å¼•å…¥æ­£åˆ™åº“
from dotenv import load_dotenv
from openai import OpenAI
from pydantic import BaseModel

from .database import SessionLocal
from .model import Word, UserWordProgress, QuizMistake
from .schemas import WordDTO, StudySubmit, ArticleDTO, QuizItem, MistakeCreate, MistakeDTO, WritingSubmit, WritingDTO
from .srs_algo import calculate_review

from .model import Article, UserStats, UserWriting # è®°å¾—å¯¼å…¥

# 1. åŠ è½½æœ¬åœ° .env æ–‡ä»¶ (å¦åˆ™è¯»ä¸åˆ° API Key)
load_dotenv()

# 2. åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
# å³ä½¿éƒ¨ç½²åˆ°äº‘ç«¯ï¼Œè¿™æ®µä»£ç ä¹Ÿå…¼å®¹ï¼ˆäº‘ç«¯ä¼šè‡ªåŠ¨æ³¨å…¥ç¯å¢ƒå˜é‡ï¼Œæœ¬åœ°åˆ™è¯»å– .envï¼‰
client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url=os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
)

class GrammarRequest(BaseModel):
    sentence: str

router = APIRouter()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def run_import_task():
    print("ğŸš€ å¼€å§‹åå°å¯¼å…¥å•è¯ä»»åŠ¡...")
    csv_path = 'scripts/ecdict.csv' # Render ä¸Šæ–‡ä»¶è·¯å¾„æ˜¯ç›¸å¯¹äºæ ¹ç›®å½•çš„
    
    if not os.path.exists(csv_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {csv_path}")
        return

    db = SessionLocal()
    try:
        with open(csv_path, newline='', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            count = 0
            for row in reader:
                tags = row.get('tag', '')
                if 'zk' in tags or 'gk' in tags: # åªå¯¼å…¥ä¸­é«˜è€ƒ
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    existing = db.query(Word).filter(Word.spell == row['word']).first()
                    if not existing:
                        word = Word(
                            spell=row['word'],
                            phonetic=row['phonetic'],
                            definition=row['definition'],
                            translation=row['translation'],
                            exchange=row['exchange'],
                            tag=tags
                        )
                        db.add(word)
                        count += 1
                        if count % 100 == 0:
                            db.commit()
                            print(f"å·²å¯¼å…¥ {count} ...")
            db.commit()
            print(f"âœ… å¯¼å…¥å®Œæˆï¼å…± {count} ä¸ªå•è¯ã€‚")
    except Exception as e:
        print(f"âŒ å¯¼å…¥å‡ºé”™: {e}")
    finally:
        db.close()

@router.get("/user/dashboard")
def get_user_dashboard(db: Session = Depends(get_db)):
    user_id = 1
    
    # 1. æ€»å…±å·²èƒŒå•è¯æ•° (is_learned = 1)
    total_learned = db.query(UserWordProgress).filter(
        UserWordProgress.user_id == user_id,
        UserWordProgress.is_learned == 1
    ).count()
    
    # 2. å‰©ä½™å¾…å¤ä¹ /æ–°è¯ (ä»Šæ—¥ä»»åŠ¡)
    # é€»è¾‘ï¼šæ‰¾å‡º next_review <= now çš„è¯ + è¿˜æ²¡èƒŒçš„æ–°è¯(è¿™é‡Œç®€å•æ¨¡æ‹Ÿä¸€ä¸‹ï¼Œå‡è®¾æ¯æ—¥å›ºå®š20ä¸ª)
    # ä¸ºäº† MVP ç®€å•å±•ç¤ºï¼Œæˆ‘ä»¬ç›´æ¥æŸ¥ "queue" æ¥å£åŒæ ·çš„é€»è¾‘ï¼Œçœ‹æœ‰å¤šå°‘ä¸ª
    today_count = 15 # æš‚æ—¶å†™ä¸ªæ¨¡æ‹Ÿæ•°æ®ï¼Œæˆ–è€…ä½ å¯ä»¥å¤ç”¨ get_study_queue çš„è®¡æ•°é€»è¾‘
    
    # 3. çœŸå®ï¼šè·å–æ‰“å¡å¤©æ•° === ä¿®æ”¹äº†è¿™é‡Œ ===
    streak_days = 0
    user_stats = db.query(UserStats).filter(UserStats.user_id == user_id).first()
    if user_stats:
        streak_days = user_stats.streak_days
    
    return {
        "total_learned": total_learned,
        "today_task": today_count,
        "streak_days": streak_days,
        "vocabulary_limit": 880 # å‡è®¾æ˜¯ä¸­è€ƒå¤§çº²è¯æ±‡é‡
    }

# 1. è·å–å­¦ä¹ é˜Ÿåˆ— (æ–°è¯ + éœ€è¦å¤ä¹ çš„æ—§è¯)
@router.get("/study/queue", response_model=List[WordDTO])
def get_study_queue(db: Session = Depends(get_db)):
    user_id = 1 # MVPå›ºå®šç”¨æˆ·
    
    # A. æ‰¾éœ€è¦å¤ä¹ çš„è¯ (next_review <= now)
    review_list = db.query(Word).join(UserWordProgress).filter(
        UserWordProgress.user_id == user_id,
        UserWordProgress.next_review <= datetime.utcnow()
    ).limit(20).all()

    # å¦‚æœå¤ä¹ è¯ä¸å¤Ÿ 10 ä¸ªï¼Œå°±åŠ ç‚¹æ–°è¯
    if len(review_list) < 10:
        limit_new = 10 - len(review_list)
        # æ‰¾è¿˜æ²¡æœ‰è¿›åº¦çš„è¯ (LEFT JOIN check)
        # å­æŸ¥è¯¢ï¼šæ‰¾å‡ºè¯¥ç”¨æˆ·å­¦è¿‡çš„ word_id
        subquery = db.query(UserWordProgress.word_id).filter(UserWordProgress.user_id == user_id)
        
        new_words = db.query(Word).filter(
            Word.id.notin_(subquery),
            Word.ai_sentence != None  # åªå‡ºæœ‰ AI ä¾‹å¥çš„è¯
        ).limit(limit_new).all()
        
        review_list.extend(new_words)

    return review_list

# 2. æäº¤å­¦ä¹ ç»“æœ
@router.post("/study/submit")
def submit_study(data: StudySubmit, db: Session = Depends(get_db)):
    user_id = 1
    
    # æŸ¥æ‰¾æˆ–åˆ›å»ºè¿›åº¦è®°å½•
    progress = db.query(UserWordProgress).filter(
        UserWordProgress.user_id == user_id,
        UserWordProgress.word_id == data.word_id
    ).first()

    if not progress:
        progress = UserWordProgress(
            user_id=user_id, 
            word_id=data.word_id, 
            easiness=2.5, 
            interval=0, 
            repetitions=0
        )
        db.add(progress)

    # è°ƒç”¨ç®—æ³•è®¡ç®—
    ef, interval, reps, next_date = calculate_review(
        data.quality, progress.easiness, progress.interval, progress.repetitions
    )

    # æ›´æ–°æ•°æ®åº“
    progress.easiness = ef
    progress.interval = interval
    progress.repetitions = reps
    progress.next_review = next_date
    progress.is_learned = 1
    
    # === å¤„ç†æ‰“å¡é€»è¾‘ ===
    today = date.today()

    # è·å–æˆ–åˆ›å»ºç”¨æˆ·ç»Ÿè®¡
    user_stats = db.query(UserStats).filter(UserStats.user_id == user_id).first()
    if not user_stats:
        user_stats = UserStats(user_id=user_id, streak_days=0, last_study_date=None)
        db.add(user_stats)

    last_date = user_stats.last_study_date.date() if user_stats.last_study_date else None

    if last_date == today:
        pass # ä»Šå¤©å·²ç»æ‰“è¿‡å¡äº†ï¼Œä¸å¤„ç†
    elif last_date == today - timedelta(days=1):
        # æ˜¨å¤©æ‰“å¡äº†ï¼Œè¿ç»­å¤©æ•°+1
        user_stats.streak_days += 1
        user_stats.last_study_date = datetime.utcnow()
    else:
        # æ–­ç­¾äº†ï¼ˆæˆ–è€…æ˜¯ç¬¬ä¸€æ¬¡ï¼‰ï¼Œé‡ç½®ä¸º1
        user_stats.streak_days = 1
        user_stats.last_study_date = datetime.utcnow()

    db.commit()
    return {"status": "ok", "next_review": next_date}

@router.get("/reading/list", response_model=List[ArticleDTO])
def get_articles(db: Session = Depends(get_db)):
    return db.query(Article).order_by(Article.id.desc()).limit(10).all()

@router.get("/reading/{article_id}", response_model=ArticleDTO)
def get_article_detail(article_id: int, db: Session = Depends(get_db)):
    article = db.query(Article).filter(Article.id == article_id).first()
    if not article:
        raise HTTPException(status_code=404, detail="Article not found")
    return article

@router.post("/reading/{article_id}/quiz", response_model=List[QuizItem])
def generate_quiz(article_id: int, db: Session = Depends(get_db)):
    # 1. æŸ¥å‡ºæ–‡ç« 
    article = db.query(Article).filter(Article.id == article_id).first()
    if not article:
        raise HTTPException(status_code=404, detail="Article not found")

    # 2. è°ƒç”¨ DeepSeek
    print(f"ğŸ¤– AIæ­£åœ¨ä¸ºæ–‡ç«  {article.title} å‡ºé¢˜...") # åŠ ä¸ªæ—¥å¿—æ–¹ä¾¿è°ƒè¯•

    prompt = f"""
    Based on the text below, create 3 multiple-choice questions for a middle school student.

    Text:
    {article.content}

    You MUST return the result as a pure JSON list.
    Strict format requirements:
    1. Do not use Markdown formatting (no ```json or ```).
    2. The root element must be a LIST [].
    3. Each item must have: "question", "options" (list of 4 strings), "answer" (just A, B, C, or D), and "explanation".

    Example:
    [
      {{
        "question": "What is the main idea?",
        "options": ["A. Idea 1", "B. Idea 2", "C. Idea 3", "D. Idea 4"],
        "answer": "A",
        "explanation": "Because..."
      }}
    ]
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1, # é™ä½éšæœºæ€§ï¼Œä¿è¯æ ¼å¼ç¨³å®š
            response_format={"type": "json_object"} # å¼ºåˆ¶ JSON
        )
        content = response.choices[0].message.content
        print(f"ğŸ¤– AIåŸå§‹è¿”å›: {content}") # æ‰“å°å‡ºæ¥çœ‹çœ‹ï¼Œå¦‚æœæŠ¥é”™æ–¹ä¾¿æ’æŸ¥

        # === å¢å¼ºå‹ JSON æ¸…æ´—é€»è¾‘ ===
        # 1. æœ‰æ—¶å€™ AI è¿˜æ˜¯ä¼šè¿”å› ```jsonï¼Œæ‰‹åŠ¨å»æ‰
        if "```" in content:
            content = content.replace("```json", "").replace("```", "")

        # 2. å°è¯•è§£æ
        data = json.loads(content)

        # 3. å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœè¿”å›çš„æ˜¯ {"quizzes": [...]} æˆ–è€…æ˜¯ {"questions": [...]}
        if isinstance(data, dict):
            for key in ["quizzes", "questions", "items"]:
                if key in data and isinstance(data[key], list):
                    return data[key]
            # å¦‚æœæ˜¯å­—å…¸ä½†æ²¡æ‰¾åˆ° keyï¼Œå¯èƒ½ç»“æ„ä¸å¯¹ï¼Œå¼ºè¡Œè½¬ list è¯•è¯•?
            # è¿™é‡Œçš„ fallback è§†æƒ…å†µè€Œå®šï¼Œé€šå¸¸ä¸Šé¢èƒ½è§£å†³

        # 4. å¦‚æœæœ¬èº«å°±æ˜¯ listï¼Œç›´æ¥è¿”å›
        if isinstance(data, list):
            return data

        raise ValueError("AI returned unexpected JSON structure")

    except Exception as e:
        print(f"âŒ AI Error Details: {e}") # è¿™ä¸€è¡Œéå¸¸é‡è¦ï¼Œçœ‹ç»ˆç«¯æŠ¥é”™
        raise HTTPException(status_code=500, detail=f"AI generation failed: {str(e)}")

# 1. æ‰¹é‡ä¿å­˜é”™é¢˜ (åœ¨æµ‹éªŒç»“ç®—æ—¶è°ƒç”¨)
@router.post("/mistakes/batch_add")
def add_mistakes(mistakes: List[MistakeCreate], db: Session = Depends(get_db)):
    user_id = 1
    for m in mistakes:
        # ç®€å•æŸ¥é‡ï¼šé˜²æ­¢åŒä¸€é“é¢˜é‡å¤å­˜ (å¯é€‰)
        exists = db.query(QuizMistake).filter(
            QuizMistake.user_id == user_id, 
            QuizMistake.question == m.question
        ).first()
        
        if not exists:
            new_mistake = QuizMistake(
                user_id=user_id,
                question=m.question,
                options=m.options,
                correct_answer=m.correct_answer,
                user_answer=m.user_answer,
                explanation=m.explanation,
                from_article_title=m.from_article_title
            )
            db.add(new_mistake)
    
    db.commit()
    return {"status": "ok", "saved_count": len(mistakes)}

# 2. è·å–æ‰€æœ‰é”™é¢˜
@router.get("/mistakes/list", response_model=List[MistakeDTO])
def get_mistakes(db: Session = Depends(get_db)):
    user_id = 1
    return db.query(QuizMistake).filter(QuizMistake.user_id == user_id).order_by(QuizMistake.id.desc()).all()

# 3. ç§»é™¤é”™é¢˜ (å·²æŒæ¡)
@router.delete("/mistakes/{mistake_id}")
def delete_mistake(mistake_id: int, db: Session = Depends(get_db)):
    db.query(QuizMistake).filter(QuizMistake.id == mistake_id).delete()
    db.commit()
    return {"status": "deleted"}

# 1. æäº¤ä½œæ–‡å¹¶è·å– AI æ‰¹æ”¹
@router.post("/writing/evaluate", response_model=WritingDTO)
def evaluate_writing(data: WritingSubmit, db: Session = Depends(get_db)):
    user_id = 1
    
    print(f"ğŸ¤– æ­£åœ¨æ‰¹æ”¹ä½œæ–‡: {data.topic}")
    prompt = f"""
    Act as an English teacher. Evaluate the following student essay.
    Topic: {data.topic}
    Student Content: {data.content}
    
    Return strict JSON (no markdown code blocks):
    {{
        "score": 85, 
        "comment": "General feedback...",
        "corrections": [
            {{"original": "wrong text", "correction": "right text", "reason": "grammar rule"}}
        ],
        "better_version": "A rewritten native-like version..."
    }}
    """
    
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.1, # é™ä½éšæœºæ€§
            timeout=60 # ç»™ DeepSeek SDK æ›´å¤šæ—¶é—´
        )
        content = response.choices[0].message.content
        print(f"ğŸ“ AIåŸå§‹è¿”å›: {content}") # æ‰“å°å‡ºæ¥æ–¹ä¾¿è°ƒè¯•

        # === å¢å¼ºå‹ JSON æ¸…æ´— ===
        if "```" in content:
            content = content.replace("```json", "").replace("```", "")
        
        feedback = json.loads(content)
        
        # === å­˜å…¥æ•°æ®åº“ ===
        writing = UserWriting(
            user_id=user_id,
            topic=data.topic,
            original_content=data.content,
            ai_feedback=feedback
        )
        db.add(writing)
        db.commit()
        db.refresh(writing)
        
        return writing

    except Exception as e:
        print(f"âŒ AI Error: {e}") # è¿™ä¸€è¡Œèƒ½è®©ä½ çœ‹åˆ°å…·ä½“æŠ¥é”™
        raise HTTPException(status_code=500, detail=f"AI evaluation failed: {str(e)}")

# 2. è·å–å†™ä½œå†å²
@router.get("/writing/history", response_model=List[WritingDTO])
def get_writing_history(db: Session = Depends(get_db)):
    user_id = 1
    return db.query(UserWriting).filter(UserWriting.user_id == user_id).order_by(UserWriting.id.desc()).all()

# 3. éšæœºç”Ÿæˆä¸€ä¸ªé¢˜ç›® (å¯é€‰å°åŠŸèƒ½)
@router.get("/writing/topic")
def get_random_topic():
    # è¿™é‡Œå¯ä»¥ç®€å•å†™æ­»å‡ ä¸ªï¼Œæˆ–è€…è®©AIç”Ÿæˆ
    topics = [
        "My Favorite Hobby",
        "A Memorable Trip",
        "The Importance of Learning English",
        "If I Had a Million Dollars",
        "My Best Friend"
    ]
    import random
    return {"topic": random.choice(topics)}

# 2. è¯­æ³•åˆ†ææ¥å£
@router.post("/grammar/analyze")
def analyze_grammar(req: GrammarRequest):
    print(f"ğŸ¤– æ­£åœ¨åˆ†æé•¿éš¾å¥: {req.sentence}")

    prompt = f"""
    You are an expert English grammar teacher. Analyze the syntax of the following sentence for a student.

    Sentence: "{req.sentence}"

    Return strict JSON (no markdown block):
    {{
      "translation": "Translate the sentence into natural Chinese.",
      "structure": [
        {{"part": "Subject (ä¸»è¯­)", "content": "The specific words", "color": "text-green-600", "bg": "bg-green-50"}},
        {{"part": "Verb (è°“è¯­)", "content": "The specific words", "color": "text-red-600", "bg": "bg-red-50"}},
        {{"part": "Object/Complement (å®¾/è¡¨)", "content": "The specific words", "color": "text-blue-600", "bg": "bg-blue-50"}},
        {{"part": "Modifier (ä¿®é¥°æˆåˆ†)", "content": "Time/Place/Clauses...", "color": "text-gray-600", "bg": "bg-gray-50"}}
      ],
      "grammar_points": [
        {{ "title": "Point name (e.g. å®šè¯­ä»å¥)", "desc": "Explanation..." }}
      ]
    }}
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.1
        )
        content = response.choices[0].message.content

        # æ¸…æ´— Markdown
        if "```" in content:
            content = content.replace("```json", "").replace("```", "")

        return json.loads(content)

    except Exception as e:
        print(f"Grammar AI Error: {e}")
        raise HTTPException(status_code=500, detail="Analysis failed")

@router.get("/word/lookup")
def lookup_word(spell: str, db: Session = Depends(get_db)):
    # å¿½ç•¥å¤§å°å†™æŸ¥æ‰¾
    word = db.query(Word).filter(Word.spell == spell.lower()).first()

    if not word:
        # å¦‚æœæ•°æ®åº“æ²¡æœ‰ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªç”Ÿåƒ»è¯æˆ–è€…å˜å½¢è¯(dolphins)
        # ç®€å•å¤„ç†ï¼šè¿”å›ç©ºï¼Œæˆ–è€…å¯ä»¥æ¥å…¥ DeepSeek å®æ—¶æŸ¥è¯¢ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
        return {"found": False, "spell": spell}

    return {
        "found": True,
        "id": word.id,
        "spell": word.spell,
        "phonetic": word.phonetic,
        "translation": word.translation,
        "definition": word.definition
    }

@router.get("/admin/trigger_import")
def trigger_import(background_tasks: BackgroundTasks):
    # ä½¿ç”¨åå°ä»»åŠ¡è¿è¡Œï¼Œé˜²æ­¢è¯·æ±‚è¶…æ—¶
    background_tasks.add_task(run_import_task)
    return {"message": "æ­£åœ¨åå°å¯¼å…¥æ•°æ®ï¼Œè¯·æŸ¥çœ‹ Render æ—¥å¿—..."}

